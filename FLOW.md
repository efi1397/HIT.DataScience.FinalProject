# 1.0 Introduction
What/How/Why we do this ML project.
Link for github page!

# 2.0 Requirements and Imports

## 2.1 Requirements

```txt
Requirements list 
```

## 2.2 Imports

```Python
imports ...
```

# 3.0 Data Acquision
Our strategy about crawling:
* iterations

## 3.1 Resources
Relevant packages/tools
chromedriver + download link for all distributions
Resources: 
* Medium
* Hubpages
* Newsbreak

### 3.1.1 Mediun
description + challenges + solutions + code + photoes
### 3.1.2 Hubpages
### 3.1.3 Newsbreak

## 3.2 Data Acquision Results
```
merged_all_csv_files ...
```
All csv files
Final csv result
 
# 4.0 Data Cleaning

## 4.1 Remove Unnecessary Columns
## 4.2 Remove Null Objects
## 4.2 Remove Duplicate Rows

# 5.0 EDA
What is EDA in text world ..

## 5.1 Merged header and content

## 5.2 Visualization Vol.1
Before cleaning ...
To understand our data we use WordCloud package ...
View data

## 5.2.1 Pie chart
## 5.2.2 WordCloud
## 5.2.3 Top 10 words
## 5.2.4 Distribution of Words
## 5.2.5 Conclustion

## 5.3 Remove Text with Short Content
## 5.4 Remove Numbers \n and \r from Text
Save for benfford rule ...
## 5.5 Remove non-english Texts
## 5.6 Remove Special Characters
## 5.7 Lemmatization and POS and Chunk and Tokenization
## 5.8 Remove Stopwords
## 5.9 Dataframe Lowercase

## 5.10 Visualization Vol.2
After cleaning ...
## 5.10.1 Pie chart
## 5.10.2 WordCloud
## 5.10.3 Top 10 words
## 5.10.4 Distribution of Words
## 5.10.5 Conclustion

## 5.11 Final EDA Conclustion
In this section we see ...


## 6.0 Vectorization and Machine Learning

## 6.1 Cross Validation

## 7.0 Project Conclustion

